#! /home/yketa/miniconda3/bin/python3.6

import os
import sys

import math

import gsd
import gsd.hoomd
import gsd.pygsd

import numpy as np

import pickle

sys.path.append('/home/yketa')
from exponents import float_to_letters
sys.path.append('/home/yketa/hoomd/colmig_DPD_P_A/data')
from readdat import *

from collections import OrderedDict

data_dir = os.environ['DATA_DIRECTORY'] if 'DATA_DIRECTORY' in os.environ else os.getcwd() # data directory

wrap_file_name = os.environ['WRAPPED_FILE'] if 'WRAPPED_FILE' in os.environ else data_dir + '/trajectory.gsd' # wrapped trajectory file (.gsd)
unwrap_file_name = os.environ['UNWRAPPED_FILE'] if 'UNWRAPPED_FILE' in os.environ else data_dir + '/trajectory.dat' # unwrapped trajectory file (binary)
parameters_file = os.environ['PARAMETERS_FILE'] if 'PARAMETERS_FILE' in os.environ else data_dir + '/param.pickle' # parameters file

dis_time = int(eval(os.environ['TIME'])) if 'TIME' in os.environ else -1 # time interval for the displacement

init_frame = int(eval(os.environ['INITIAL_FRAME'])) if 'INITIAL_FRAME' in os.environ else -1 # initial time for the calculation of the displacement correlation
int_max = int(eval(os.environ['INTERVAL_MAXIMUM'])) if 'INTERVAL_MAXIMUM' in os.environ else 1 # maximum number of intervals taken for the calculation of the displacement correlation

r_max = float(eval(os.environ['R_MAX'])) if 'R_MAX' in os.environ else 20 # maximum radius for 2D plot

with open(parameters_file, 'rb') as param_file:
	N, a, pdi, N_sizes, density, box_size, kT, mu, k, vzero, dr, damp_bro, shear_rate, time_step, N_steps, period_dump, prep_steps = pickle.load(param_file)

r_cut = a*float(eval(os.environ['R_CUT'])) if 'R_CUT' in os.environ else a*2 # cutoff radius for coarse graining function
sigma = float(eval(os.environ['SIGMA'])) if 'SIGMA' in os.environ else r_cut # length scale of the spatial extent of the coarse graining function

prep_frames = math.ceil(prep_steps/period_dump) # number of preparation frames

Ncases = int(eval(os.environ['N_CASES'])) if 'N_CASES' in os.environ else int(np.sqrt(N)) + (1 - int(np.sqrt(N))%2) # number of cases in each direction to compute the velocity grid
grid_points = np.array([(x, y) for x in np.linspace(- box_size/2, box_size/2, Ncases, endpoint=False) for y in np.linspace(- box_size/2, box_size/2, Ncases, endpoint=False)])

Nentries = N_steps//period_dump # number of time snapshots in velocity and position files
init_frame = int(Nentries/2) if init_frame < 0 else init_frame # initial frame
Nframes = Nentries - init_frame # number of frames available for the calculation

dis_time = Nframes + dis_time if dis_time <= 0 else dis_time # length of the interval of time for which the correlation displacement is calculated

times = np.array(list(OrderedDict.fromkeys(map(lambda x: int(x), np.linspace(init_frame, Nentries - dis_time - 1, int_max))))) # frames at which to calculate the correlation displacement

def CGf(r, sigma, r_cut):
	# coarse graining function
	if r > r_cut:
		return 0
	else:
		Dg = 2*np.pi*(sigma**2)*(1 - np.exp(-0.5*((r_cut/sigma)**2))) # normalisation factor
		return np.exp(-0.5*((r/sigma)**2))/Dg

def within_rcut(point, positions, r_cut):
	# returns indexes of positions in positions whose within a distance
	# r_cut of point
	return np.where(np.sqrt(np.sum((positions - point)**2, axis=1)) < r_cut)[0]

def Averaging(point, var, positions, sigma, r_cut):
	# averages the particle dependent variable var at point with coarse
	# graining function with parameters sigma and r_cut
	return np.sum(list(map(lambda index: var[index]*CGf(np.sqrt(np.sum((positions[index] - point)**2)), sigma, r_cut), within_rcut(point, poisitons, r_cut))))

def Adensity(point, positions, sigma, r_cut):
	# average densition at point point
	return Averaging(point, np.full(len(positions), fill_value=1), sigma, r_cut)

def strain(point, time, dis_time, prep_frames, w_traj, unwrap_file, sigma, r_cut):
	# returns strain at point point

	positions = w_traj[int(prep_frames + time + (dis_time if 'ENDPOINT' in os.environ and eval(os.environ['ENDPOINT']) else 0))].particles.position[:, :2] # position at time time (with boundary conditions)

	pos0 = getarray(unwrap_file, N, time) # positions at time time (without periodic boundary conditions)
	pos1 = getarray(unwrap_file, N, time + dis_time) # position at time time + dis_time (without periodic boundary conditions)
	displacements = pos1 - pos0 # displacements of the particles between time and time + dis_time

	rho = Adensity(point, positions, sigma, r_cut) # averaged density
	Ax = Averaging(point, positions[:, 0], positions, sigma, r_cut)
	Auy = Averaging(point, displacements[:, 1], positions, sigma, r_cut)
	Ay = Averaging(point, positions[:, 1], positions, sigma, r_cut)
	Aux = Averaging(point, displacements[:, 0], positions, sigma, r_cut)
	Auxy = Averaging(point, displacements[:, 0]*positions[:, 1], positions, sigma, r_cut)
	Auyx = Averaging(point, displacements[:, 1]*positions[:, 0], positions, sigma, r_cut)

	return 0.5*(-(Ax*Auy + Ay*Aux)/((rho*sigma)**2) + (Auxy + Auyx)/(rho*(sigma**2)))

def strain_grid(Ncases, grid_points, time, dis_time, prep_frames, wrap_file, unwrap_file, sigma, r_cut):
	# returns strain Ncases x Ncases grid associated to the box

	w_traj = gsd.hoomd.HOOMDTrajectory(wrap_file);
	grid = list(map(lambda point: strain(point, time, dis_time, prep_frames, w_traj, unwrap_file, sigma, r_cut), grid_points))

	return np.reshape(grid, (Ncases, Ncases))

def corField(field):
	# Return 2D correlation of a scalar field.

	FFT = np.fft.fft2(field)
	C = np.real(np.fft.ifft2(np.conj(FFT)*FFT))
	Norm = np.sum(field**2)

	return C, Norm

with gsd.pygsd.GSDFile(open(wrap_file_name, 'rb')) as wrap_file, open(unwrap_file_name, 'rb') as unwrap_file:
	Sgrid = list(map(lambda time: strain_grid(Ncases, grid_points, time, dis_time, prep_frames, wrap_file, unwrap_file, sigma, r_cut), times))

Css2D = (lambda C, Norm: C/Norm)(*tuple(np.sum(list(map(corField, Sgrid)), axis=0))) # 2D strain correlation

# SAVING

filename = lambda var: data_dir + str('/' + var + ('b' if not('ENDPOINT' in os.environ and eval(os.environ['ENDPOINT'])) else '') + '_D%s_V%s_R%s_N%s_I%s_T%s_M%s_C%s.pickle' % tuple(map(float_to_letters, [density, vzero, dr, N, init_frame, dis_time, int_max, Ncases]))) # filename

with open(filename('Css'), 'wb') as Css_dump_file:
	pickle.dump(Css2D, Css_dump_file)

# PLOT

if 'SHOW' in os.environ and eval(os.environ['SHOW']):

	import matplotlib as mpl
	import matplotlib.pyplot as plt
	import matplotlib.colors as colors
	import matplotlib.cm as cmx
	from mpl_toolkits.axes_grid1 import make_axes_locatable
	from matplotlib.gridspec import GridSpec
	cmap = plt.cm.jet

	fig, ax = plt.subplots()

	fig.set_size_inches(16, 16)
	fig.subplots_adjust(wspace=0.3)
	fig.subplots_adjust(hspace=0.3)

	fig.suptitle(r'$N=%.2e, \phi=%1.2f, \tilde{v}=%.2e, \tilde{\nu}_r=%.2e$' % (N, density, vzero, dr) + '\n' + r'$S_{init}=%.2e, \Delta t=%.2e, S_{max}=%.2e, N_{cases}=%.2e$' % (init_frame, dis_time*period_dump*time_step, int_max, Ncases))

	# Css2D

	C = 'C_{\epsilon_{xy}\epsilon_{xy}}'

	Cmin = np.min(Css2D)
	Cmax = np.max(Css2D)

	CvNorm = colors.Normalize(vmin=Cmin, vmax=Cmax)
	CscalarMap = cmx.ScalarMappable(norm=CvNorm, cmap=cmap)

	r_max_cases = int(r_max*(box_size/Ncases))
	C2D_display = np.roll(np.roll(Css2D, int(Ncases/2), axis=0), int(Ncases/2), axis=1)[int(Ncases/2) - r_max_cases:int(Ncases/2) + r_max_cases + 1, int(Ncases/2) - r_max_cases:int(Ncases/2) + r_max_cases + 1]

	axs[0, 0].imshow(C2D_display, cmap=cmap, norm=CvNorm, extent=[-r_max, r_max, -r_max, r_max])

	axs[0, 0].set_xlabel(r'$x$')
	axs[0, 0].set_ylabel(r'$y$')
	axs[0, 0].set_title('2D ' + r'$%s$' % C)

	divider = make_axes_locatable(axs[0, 0])
	cax = divider.append_axes("right", size="5%", pad=0.05)
	cb = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=CvNorm, orientation='vertical')
	cb.set_label(r'$%s$' % C, labelpad=20, rotation=270)

	plt.show()
